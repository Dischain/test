{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 4\n",
    "VALIDATION_SPLIT = .2\n",
    "IMG_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../Downloads/aptos2019-blindness-detection/'\n",
    "train_imgs_path = os.path.join(dataset_path, 'train_images/')\n",
    "test_imgs_path = os.path.join(dataset_path, 'test_images/')\n",
    "\n",
    "train_df_path = os.path.join(dataset_path, 'train.csv')\n",
    "test_df_path = os.path.join(dataset_path, 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentBloodVessels(object):       \n",
    "    def __call__(self, image):\n",
    "        # 1. Extract green channel\n",
    "        img_green = image[:, :, 1]\n",
    "\n",
    "        # 2. Use median 3x3 filter\n",
    "        img_median_filtered = cv2.medianBlur(img_green, 3)\n",
    "        # 3. Adaptive thresholding\n",
    "        img_thresholded_adaptive = \\\n",
    "            cv2.adaptiveThreshold(img_median_filtered, 127, \\\n",
    "                                  cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \\\n",
    "                                  cv2.THRESH_BINARY, 97, 1)\n",
    "\n",
    "        # 4. Use contrast limited adaptive histogram equalisation\n",
    "        clahe = cv2.createCLAHE(clipLimit=5, tileGridSize=(3, 3))\n",
    "        img_clahed = clahe.apply(img_thresholded_adaptive)\n",
    "\n",
    "        # 5. Denoising by median blur\n",
    "        img_denoised_blur = cv2.medianBlur(img_clahed, 11)\n",
    "        # 5. Denoising by Wiener filter\n",
    "        # psf = np.ones((5, 5)) / 25\n",
    "        # # img_clahed = cv2.filter2D(img_clahed, psf)\n",
    "        # img_denoised = np.uint8(restoration.wiener(img_clahed, psf, 200))\n",
    "\n",
    "        # 6. Otsu thresholding\n",
    "        ret2, img_otsu_thresholded = cv2.threshold(img_denoised_blur, 0, 255, \\\n",
    "                                                   cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "        # 7. Suppress noise by using morphological opening by\n",
    "        #    circular SE with radius 8 px\n",
    "        kernel_circ = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9))\n",
    "        img_opened = cv2.morphologyEx(img_otsu_thresholded, cv2.MORPH_OPEN, kernel_circ)\n",
    "        \n",
    "        return img_opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the grayscaled image in a sample.\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceRadius(object):\n",
    "    \"\"\" Reduce radius of RGB image\n",
    "    \"\"\"\n",
    "    def __call__(self, image):       \n",
    "        h, w, c = image.shape\n",
    "        frame0 = np.zeros((h, w, c), dtype = np.uint8)\n",
    "\n",
    "        cv2.circle(frame0, (int(np.floor(w / 2)), int(np.floor(h / 2))),\n",
    "                           int(np.floor((h * 96) / float(2 * 100))), (255, 255, 255), -1)\n",
    "\n",
    "        frame1 = cv2.cvtColor(frame0, cv2.COLOR_BGR2GRAY)\n",
    "        res = cv2.bitwise_and(image, image, mask=frame1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeFundus(object):\n",
    "    def __init__(self, dim, interpolation=cv2.INTER_AREA):\n",
    "        self.dim = dim\n",
    "        self.interpolation = interpolation\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        ret, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_OTSU)\n",
    "        contours, hierarchy = cv2.findContours(thresh, 1, 2)\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # resized = resize_img_adaptively(img_segmented, cnt, (256, 256))\n",
    "\n",
    "        (x,y), radius = cv2.minEnclosingCircle(cnt)\n",
    "\n",
    "        x = int(x); y = int(y); radius = int(radius)\n",
    "\n",
    "        if x < radius:\n",
    "            radius = x\n",
    "        if y < radius:\n",
    "            radius = y\n",
    "\n",
    "        crop_img = image[y - radius:y + radius, x - radius:x + radius]\n",
    "\n",
    "        resized = cv2.resize(crop_img, self.dim, interpolation=self.interpolation)\n",
    "\n",
    "        return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveBoundigCircle(object):\n",
    "    def __init__(self, shift):\n",
    "        self.shift = shift\n",
    "        \n",
    "    def __call__(self, image):        \n",
    "        dim = image.shape\n",
    "        mask = np.zeros(dim, np.uint8)\n",
    "\n",
    "        circle = cv2.circle(mask, (dim[0]//2, dim[1]//2), dim[1]//2 - self.shift, 1, thickness=-1)\n",
    "\n",
    "        return cv2.bitwise_and(image, circle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, image):\n",
    "        return torch.from_numpy(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_image(im):\n",
    "    # Compute the center (cx, cy) and radius of the eye\n",
    "    cy = np.uint8(im.shape[0] // 2)\n",
    "    midline = im[cy,:]\n",
    "    midline = np.where(midline>midline.mean() / 3)[0]\n",
    "\n",
    "    if len(midline)>im.shape[1] // 2:\n",
    "        x_start, x_end = np.min(midline), np.max(midline)\n",
    "    else: # This actually rarely happens p~1/10000\n",
    "        x_start, x_end = im.shape[1] // 10, 9 * im.shape[1] // 10\n",
    "    cx = np.uint8((x_start + x_end) / 2)\n",
    "    r = np.uint8((x_end - x_start) / 2)\n",
    "    return cx, cy, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, phase, transform=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):            \n",
    "        current_img_path = os.path.join(self.root_dir,\n",
    "                                self.df.iloc[idx, 0])\n",
    "        \n",
    "        img = cv2.imread(current_img_path  + '.png')\n",
    "        \n",
    "        label = self.df.iloc[idx, 1]                \n",
    "        \n",
    "        if self.transform:\n",
    "            if self.phase == 'train':\n",
    "                img = self.transform['train'](img)                \n",
    "            elif self.phase == 'val' or self.phase == 'test':\n",
    "                img = self.transform['val'](img)\n",
    "            \n",
    "        return img, label        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        ReduceRadius(),\n",
    "        SegmentBloodVessels(),\n",
    "        ResizeFundus((IMG_SIZE, IMG_SIZE)),\n",
    "        RemoveBoundigCircle(10),\n",
    "        RandomCrop(224),        \n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        ReduceRadius(),\n",
    "        SegmentBloodVessels(),\n",
    "        ResizeFundus((IMG_SIZE, IMG_SIZE)),\n",
    "        RemoveBoundigCircle(10),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_df_path)[0:50]\n",
    "data_size = df.shape[0]\n",
    "train_df, val_df = train_test_split(df, test_size=VALIDATION_SPLIT, \n",
    "                                    stratify=df['diagnosis'])\n",
    "\n",
    "train_dataset = DRDataset(df=train_df, root_dir=train_imgs_path,\n",
    "                          phase='train', transform=data_transforms)\n",
    "\n",
    "val_dataset = DRDataset(df=val_df, root_dir=train_imgs_path,\n",
    "                        phase='val', transform=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, \\\n",
    "                         num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_df_path)[0:50]\n",
    "data_size = df.shape[0]\n",
    "indices = list(range(data_size))\n",
    "split = int(np.floor(VALIDATION_SPLIT * data_size))\n",
    "\n",
    "train_indices, valid_indices = indices[split:], indices[:split]\n",
    "train_sampler = sampler.SubsetRandomSampler(train_indices)\n",
    "valid_sampler = sampler.SubsetRandomSampler(valid_indices)\n",
    "\n",
    "train_dataset = DRDataset(df=train_df, root_dir=train_imgs_path,\n",
    "                          phase='train', transform=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                         sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([224, 224])\n",
      "torch.Size([224, 224])\n",
      "torch.Size([224, 224])\n",
      "torch.Size([224, 224])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-8109153ae069>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#         current_batch[a] = train_dataset[i]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'length' is not defined"
     ]
    }
   ],
   "source": [
    "indexes = np.arange(data_size)\n",
    "dataset_length_by_batches = np.uint16(np.ceil(data_size / BATCH_SIZE))\n",
    "\n",
    "for idx in range(dataset_length_by_batches):\n",
    "    current_batch = np.zeros(4)\n",
    "    a = 0\n",
    "    for i in indexes[idx*BATCH_SIZE : (idx+1)*BATCH_SIZE]:\n",
    "        print(train_dataset[i][0].shape)\n",
    "#         current_batch[a] = train_dataset[i]\n",
    "        a += 1\n",
    "    print(len(current_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.zeros(4)\n",
    "arr[1] = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-env] *",
   "language": "python",
   "name": "conda-env-torch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
